{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate analytical functions with Tensorflow\n",
    "\n",
    "Purpose:\n",
    "* A simple program to approximate Analytical Functions\n",
    "with DNN & TF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 12:47:00.981467: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-24 12:47:00.981585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-24 12:47:01.027426: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-24 12:47:01.127643: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 12:47:02.296774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import math\n",
    "from random import uniform\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "\n",
    "dim_x = 2\n",
    "dim_y = 1\n",
    "no_samples = 10000\n",
    "filename = \"mymodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Training data ##############\n",
    "## x coord\n",
    "aPnts = np.empty([no_samples, dim_x])  \n",
    "for iI in range(no_samples):\n",
    "    for iJ in range(dim_x):\n",
    "        aPnts[iI][iJ] = uniform(-1.0, 1.0)\n",
    "data = aPnts #np.random.random((no_samples, dim_x))\n",
    "\n",
    "## y value\n",
    "aTres = np.empty([no_samples,])\n",
    "for iI in range(no_samples):\n",
    "    aTres[iI] = math.cos(0.5 * math.pi * aPnts[iI][0]) * math.cos(0.5 * math.pi * aPnts[iI][1])\n",
    "labels = aTres #np.random.random((no_samples,dim_y ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Test data ###################\n",
    "\n",
    "aPnts2 = np.empty([no_samples, dim_x])  \n",
    "for iI in range(no_samples):\n",
    "    for iJ in range(dim_x):\n",
    "        aPnts2[iI][iJ] = uniform(-1.0, 1.0)\n",
    "data2 = aPnts2 #np.random.random((no_samples, dim_x))\n",
    "\n",
    "## y value\n",
    "aTres2 = np.empty([no_samples,])\n",
    "for iI in range(no_samples):\n",
    "    aTres2[iI] = math.cos(0.5 * math.pi * aPnts2[iI][0]) * math.cos(0.5 * math.pi * aPnts2[iI][1])\n",
    "labels2 = aTres2 #np.random.random((no_samples,dim_y ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Model ####################################\n",
    "model = tf.keras.Sequential([\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "layers.Dense(64, activation='relu', input_shape=(dim_x,)),\n",
    "# Add another:\n",
    "layers.Dense(64, activation='relu'),\n",
    "# Add an output layer with 10 output units:\n",
    "layers.Dense(dim_y)])\n",
    "\n",
    "# Configure a model for mean-squared error regression.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss='mse',       # mean squared error\n",
    "              metrics=['mae'])  # mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0059 - mae: 0.0305\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.7282e-04 - mae: 0.0102\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.5821e-04 - mae: 0.0116\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.3998e-04 - mae: 0.0088\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.6116e-04 - mae: 0.0092\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.8646e-04 - mae: 0.0127\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.1732e-04 - mae: 0.0085\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.5600e-04 - mae: 0.0124\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.8339e-04 - mae: 0.0101\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 9.1849e-05 - mae: 0.0072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f730873b910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 1.5903e-04 - mae: 0.0097 - 342ms/epoch - 1ms/step\n",
      "\n",
      "Test accuracy: 0.00965808890759945\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(data2,  labels2, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict individual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n",
      " point to test\n",
      "-0.7142146426558194    -0.8596113282000879\n",
      "NN prediction:  [0.10459869] Analytical solution 0.0949293054604226 difference [0.00966938]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(data2)\n",
    "x = data2[0]\n",
    "\n",
    "x1 = x[0,]\n",
    "y1 = x[1,]\n",
    "print(\" point to test\")\n",
    "print (x[0,], \"  \",x[1,])\n",
    "#x.shape\n",
    "#print(x.shape, data2.shape)\n",
    "\n",
    "## Analytical solution:\n",
    "res = math.cos(0.5 * math.pi * x1) * math.cos(0.5 * math.pi * y1)\n",
    "print(\"NN prediction: \" , predictions[0], \"Analytical solution\", res, \"difference\" ,abs(predictions[0]-res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second model with gradient tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.20812422037124634\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.00012190269626444206\n",
      "Seen so far: 12864 samples\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.0007948163547553122\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.000352168339304626\n",
      "Seen so far: 12864 samples\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.00015973587869666517\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.0002913200587499887\n",
      "Seen so far: 12864 samples\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.0004951511509716511\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 8.653287659399211e-05\n",
      "Seen so far: 12864 samples\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 6.681459490209818e-05\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 7.516606274293736e-05\n",
      "Seen so far: 12864 samples\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.0003778417012654245\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.00010433907300466672\n",
      "Seen so far: 12864 samples\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.00010343497706344351\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 8.243530464824289e-05\n",
      "Seen so far: 12864 samples\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 8.312951831612736e-05\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 8.677344885654747e-05\n",
      "Seen so far: 12864 samples\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.0006261839298531413\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.00012080401938874274\n",
      "Seen so far: 12864 samples\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 7.248859765240923e-05\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 7.007949170656502e-05\n",
      "Seen so far: 12864 samples\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(dim_x,), name='x-coord')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(dim_y, name='predictions')(x)\n",
    "model2 = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables autodifferentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      # Run the forward pass of the layer.\n",
    "      # The operations that the layer applies\n",
    "      # to its inputs are going to be recorded\n",
    "      # on the GradientTape.\n",
    "      logits = model2(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "      # Compute the loss value for this minibatch.\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(loss_value, model2.trainable_weights)\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients(zip(grads, model2.trainable_weights))\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    if step % 200 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 64))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict individual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 793us/step\n",
      " point to test\n",
      "-0.7142146426558194    -0.8596113282000879\n",
      "NN prediction:  [0.09218344] Analytical solution 0.0949293054604226 difference [0.00274587]\n"
     ]
    }
   ],
   "source": [
    "predictions2 = model2.predict(data2)\n",
    "x = data2[0]\n",
    "\n",
    "x1 = x[0,]\n",
    "y1 = x[1,]\n",
    "print(\" point to test\")\n",
    "print (x[0,], \"  \",x[1,])\n",
    "#x.shape\n",
    "#print(x.shape, data2.shape)\n",
    "\n",
    "## Analytical solution:\n",
    "res = math.cos(0.5 * math.pi * x1) * math.cos(0.5 * math.pi * y1)\n",
    "print(\"NN prediction: \" , predictions2[0], \"Analytical solution\", res, \"difference\" ,abs(predictions2[0]-res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
